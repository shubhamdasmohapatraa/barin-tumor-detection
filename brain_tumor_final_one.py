# -*- coding: utf-8 -*-
"""brain tumor final one.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L9HEj9LVCV4EvGq2wJxa4T9KVkX_-Xqq
"""

from google.colab import files

uploaded = files.upload()

import zipfile
import io

zip_ref = zipfile.ZipFile(io.BytesIO(uploaded['archive.zip']), "r") # Change variable name to zip_ref
zip_ref.extractall() # Call extractall on the ZipFile instance

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.utils import shuffle
import cv2
import imutils
import numpy as np
import matplotlib.pyplot as plt
import time
from os import listdir

# %matplotlib inline

import os

# Define the paths to the directories
tumor_dir = '/content/yes'
non_tumor_dir = '/content/no'

# Count the number of images in each directory
num_tumor_images = len(os.listdir(tumor_dir))
num_non_tumor_images = len(os.listdir(non_tumor_dir))

print(f'There are {num_tumor_images} tumor images.')
print(f'There are {num_non_tumor_images} non-tumor images.')

import cv2
import os
from PIL import Image
import numpy as np
import pandas as pd

image_directory = '/content/brain_tumor_dataset/'
no_tumor_images = os.listdir(image_directory + 'no')
yes_tumor_images = os.listdir(image_directory + 'yes')

img = cv2.imread(image_directory + "yes/" + "Y10.jpg") # Fix the file path
if img is not None: # Check if image loaded successfully
  print(img.shape)
else:
  print("Failed to load image. Check the file path.")

def timing(sec_elapsed):
    h = int(sec_elapsed / (60*60))
    m = int(sec_elapsed % (60*60) / 60)
    s = sec_elapsed % 60
    return f"{h}:{m}:{s}"

def augmented_data(file_dir, n_generated_samples, save_to_dir):
    data_gen = ImageDataGenerator(rotation_range=10,
                      width_shift_range=0.1,
                      height_shift_range=0.1,
                      shear_range=0.1,
                      brightness_range=(0.3, 1.0),
                      horizontal_flip=True,
                      vertical_flip=True,
                      fill_mode='nearest')

    # Create the save directory if it doesn't exist
    os.makedirs(save_to_dir, exist_ok=True)

    for filename in os.listdir(file_dir):
        image = cv2.imread(file_dir + '/' + filename)
        image = image.reshape((1,) + image.shape)
        save_prefix = 'aug_' + filename[:-4]
        i=0
        for batch in data_gen.flow(x = image, batch_size = 1, save_to_dir = save_to_dir, save_prefix = save_prefix, save_format = "jpg"):
            i+=1
            if i>n_generated_samples:
                break

import time
start_time = time.time()

yes_path = '/content/yes'
no_path = '/content/no'

augmented_data_path = 'augmented_data/'

augmented_data(file_dir = yes_path, n_generated_samples=6, save_to_dir=augmented_data_path+'yes')
augmented_data(file_dir = no_path, n_generated_samples=9, save_to_dir=augmented_data_path+'no')

end_time = time.time()
execution_time = end_time - start_time
print(timing(execution_time))

def data_summary(main_path):
    yes_path = "/content/augmented_data/yes"
    no_path = "/content/augmented_data/no"

    n_pos = len(os.listdir(yes_path))
    n_neg = len(os.listdir(no_path))

    n = (n_pos + n_neg)

    pos_per = (n_pos*100)/n
    neg_per = (n_neg*100)/n

    print(f"Number of sample: {n}")
    print(f"{n_pos} Number of positive sample in percentage: {pos_per}%")
    print(f"{n_neg} Number of negative sample in percentage: {neg_per}%")

data_summary(augmented_data_path)

import imageio
import matplotlib.pyplot as plt
import os
import numpy as np

image_directory = "/content/augmented_data/yes"  # Directory containing images

# Iterate over images in the directory
for filename in os.listdir(image_directory):
    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Adjust file extensions as needed
        image_path = os.path.join(image_directory, filename)
        image = imageio.imread(image_path)

        # Check if the image is grayscale and convert if necessary
        if image.ndim == 2:
            image = np.stack((image,)*3, axis=-1)  # Convert to 3 channels (RGB)

        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))

        red_image = image.copy()
        red_image[:, :, 1] = 0
        red_image[:, :, 2] = 0

        green_image = image.copy()
        green_image[:, :, 0] = 0
        green_image[:, :, 2] = 0

        blue_image = image.copy()
        blue_image[:, :, 0] = 0
        blue_image[:, :, 1] = 0

        axes[0].imshow(red_image)
        axes[1].imshow(green_image)
        axes[2].imshow(blue_image)

        plt.show()

import os
import cv2

# Initialize lists to store labels, widths, and heights
no_img_label = []
no_img_width = []
no_img_height = []

# Assuming `no_tumor_images` is a list of image names
no_tumor_images = os.listdir(image_directory + "no/")

# Loop through each image
for i, image_name in enumerate(no_tumor_images):
    # Process only image files with known extensions
    if image_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')):
        # Extract the label (file name without extension)
        no_img_label.append(os.path.splitext(image_name)[0])

        # Construct the full path to the image
        img_path = os.path.join(image_directory, "no", image_name)

        # Read the image using OpenCV
        img = cv2.imread(img_path)

        # Ensure the image is read correctly
        if img is not None:
            # Get image dimensions (width and height)
            no_img_width.append(img.shape[1])  # Width
            no_img_height.append(img.shape[0])  # Height
        else:
            print(f"Warning: Could not read image {img_path}")

# Example: Print the first few results
print("Labels:", no_img_label[:5])
print("Widths:", no_img_width[:5])
print("Heights:", no_img_height[:5])

no_info = {"image_label":no_img_label, "image_width":no_img_width, "image_height":no_img_height}
no_images_info_df = pd.DataFrame(no_info)
no_images_info_df

no_images_info_df.describe()

no_images_info_df[["image_width", "image_height"]].hist(figsize=(14,6), bins=10, color='#32a852')

import os
import cv2
import pandas as pd

# Initialize lists to store labels, widths, and heights for "yes" tumor images
yes_img_label = []
yes_img_width = []
yes_img_height = []

# Assuming `yes_tumor_images` is a list of image names in the "yes" directory
yes_tumor_images = os.listdir(image_directory + "yes/")

# Loop through each image
for i, image_name in enumerate(yes_tumor_images):
    # Process only image files with known extensions
    if image_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')):
        # Extract the label (file name without extension)
        yes_img_label.append(os.path.splitext(image_name)[0])

        # Construct the full path to the image
        img_path = os.path.join(image_directory, "yes", image_name)

        # Read the image using OpenCV
        img = cv2.imread(img_path)


        if img is not None:
            # Get image dimensions (width and height)
            yes_img_width.append(img.shape[1])  # Width
            yes_img_height.append(img.shape[0])  # Height
        else:
            print(f"Warning: Could not read image {img_path}")

# Create a DataFrame to store the image information
yes_info = {
    "image_label": yes_img_label,
    "image_width": yes_img_width,
    "image_height": yes_img_height
}
yes_images_info_df = pd.DataFrame(yes_info)

# Display the DataFrame
yes_images_info_df.head()

no_images_info_df[["image_width", "image_height"]].hist(figsize=(14,6), bins=10, color='#A53143')

yes_images_info_df.describe()

import cv2
import os
import tensorflow as tf
from PIL import Image
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import normalize
from tensorflow.keras import Sequential
from tensorflow.keras.layers import (Conv2D, MaxPooling2D,
     Activation, Dropout, Flatten, Dense)
from tensorflow.keras.utils import to_categorical

import os
import cv2
import numpy as np
from PIL import Image

# Directory containing the brain tumor images
image_directory = '/content/augmented_data'

# Lists of image files for 'no tumor' and 'yes tumor' classes
# Use os.path.join to correctly construct the paths
no_tumor_images = os.listdir(os.path.join(image_directory, 'no'))  # Add path separator
yes_tumor_images = os.listdir(os.path.join(image_directory, 'yes')) # Add path separator

dataset = []
label = []

# Set the input image size
INPUT_SIZE = 64

# Process 'no tumor' images
for image_name in no_tumor_images:
    if image_name.endswith('.jpg'):
        # Use os.path.join to construct the full image path
        image_path = os.path.join(image_directory, "no", image_name)  # Construct full path
        image = cv2.imread(image_path)

        # Apply Gaussian filter
        image = cv2.GaussianBlur(image, (5, 5), 0)

        # Apply Median filter
        image = cv2.medianBlur(image, 5)

        # Apply Bilateral filter
        image = cv2.bilateralFilter(image, 9, 75, 75)

        # Convert to PIL image and resize
        image = Image.fromarray(image, "RGB")
        image = image.resize((INPUT_SIZE, INPUT_SIZE))

        # Append to dataset and label
        dataset.append(np.array(image))
        label.append(0)

# Process 'yes tumor' images
for image_name in yes_tumor_images:
    if image_name.endswith('.jpg'):
        # Use os.path.join to construct the full image path
        image_path = os.path.join(image_directory, "yes", image_name) # Construct full path
        image = cv2.imread(image_path)

        # Apply Gaussian filter
        image = cv2.GaussianBlur(image, (5, 5), 0)

        # Apply Median filter
        image = cv2.medianBlur(image, 5)

        # Apply Bilateral filter
        image = cv2.bilateralFilter(image, 9, 75, 75)

        # Convert to PIL image and resize
        image = Image.fromarray(image, "RGB")
        image = image.resize((INPUT_SIZE, INPUT_SIZE))

        # Append to dataset and label
        dataset.append(np.array(image))
        label.append(1)

# Plotting Accuracy and Loss of the model
def plot_metrics(history):

    train_loss = history['loss']
    val_loss = history['val_loss']
    train_acc = history['accuracy']
    val_acc = history['val_accuracy']

    # Loss
    plt.figure()
    plt.plot(train_loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.title('Loss')
    plt.legend()
    plt.show()

    # Accuracy
    plt.figure()
    plt.plot(train_acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.title('Accuracy')
    plt.legend()
    plt.show()

dataset= np.array(dataset)
label= np.array(label)

x_train, x_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=0)

# Categorical Cross Entropy
y_train = to_categorical(y_train, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)

# Model Building
model_1 = Sequential()

# first layer
model_1.add(Conv2D(filters= 32, kernel_size=(3,3), input_shape=(INPUT_SIZE, INPUT_SIZE, 3)))
model_1.add(Activation("relu"))
model_1.add(MaxPooling2D(pool_size=(2,2)))

# 1st hidden layer
model_1.add(Conv2D(filters= 32, kernel_size=(3,3), kernel_initializer="he_uniform"))
model_1.add(Activation("relu"))
model_1.add(MaxPooling2D(pool_size=(2,2)))

# 2nd hidden layer
model_1.add(Conv2D(filters= 64, kernel_size=(3,3), kernel_initializer="he_uniform"))
model_1.add(Activation("relu"))
model_1.add(MaxPooling2D(pool_size=(2,2)))

# Flatten Layer
model_1.add(Flatten())
model_1.add(Dense(64))
model_1.add(Activation("relu"))
model_1.add(Dropout(0.5))

#Categorical Cross Entropy = 2, softmax
model_1.add(Dense(2))
model_1.add(Activation("softmax"))
model_1.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

model_1.summary()

model_1.fit(x_train, y_train, batch_size=32, verbose=True, epochs=10,
          validation_data=(x_test, y_test), shuffle=False)

model_1.save("BrainTumorCategorical10Epochs.h5")

# Binary Cross Entropy with Sigmoid Function.
dataset= np.array(dataset)
label= np.array(label)

x_train, x_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=0)

x_train = normalize(x_train, axis=1)
x_test = normalize(x_test, axis=1)

model_2 = Sequential()

# first layer
model_2.add(Conv2D(filters= 32, kernel_size=(3,3), input_shape=(INPUT_SIZE, INPUT_SIZE, 3)))
model_2.add(Activation("relu"))
model_2.add(MaxPooling2D(pool_size=(2,2)))

# 1st hidden layer
model_2.add(Conv2D(filters= 32, kernel_size=(3,3), kernel_initializer="he_uniform"))
model_2.add(Activation("relu"))
model_2.add(MaxPooling2D(pool_size=(2,2)))

# 2nd hidden layer
model_2.add(Conv2D(filters= 64, kernel_size=(3,3), kernel_initializer="he_uniform"))
model_2.add(Activation("relu"))
model_2.add(MaxPooling2D(pool_size=(2,2)))

# Flatten Layer
model_2.add(Flatten())
model_2.add(Dense(64))
model_2.add(Activation("relu"))
model_2.add(Dropout(0.5))

# Binary Cross Entropy = 1, sigmoid
model_2.add(Dense(1))
model_2.add(Activation("sigmoid"))
model_2.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


model_2.fit(x_train, y_train, batch_size=32, verbose=True, epochs=20,
          validation_data=(x_test, y_test), shuffle=False)

model_2.summary()

model_2.fit(x_train, y_train, batch_size=32, verbose=True, epochs=20,
          validation_data=(x_test, y_test), shuffle=False)

model_2.save("BrainTumor10Epochs.h5")

history_2 = model_2.history.history
plot_metrics(history_2)

model_2.save("BrainTumor10Epochss.h5")

